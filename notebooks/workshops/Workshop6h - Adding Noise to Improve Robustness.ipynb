{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCT4052 Workshop 6h: Adding Noise to Improve Robustness\n",
    "\n",
    "*Author: Stefano Fasciani, stefano.fasciani@imv.uio.no, Department of Musicology, University of Oslo.*\n",
    "\n",
    "When working with a small dataset overfitting is likely to happen. Noise can help the ML model to learn a better generalization of the data and therefore avoid overfitting and improve the accuracy on the test set.\n",
    "\n",
    "This is something we have to do \"manually\", and therefore it's essential to have all aspects of the training \n",
    "process under control. In this example we double the size of the training set by adding gaussian noise (zero mean, arbitrary variance) to the existing examples. At first we do this on a single split, and then we repeat the same on a repeated stratified k fold manually (but we have to do it manually). The variance is a parameter we have to tune. If features are scaled (normailzed) a 0.1 variance can represent 10% of the variance of the features.\n",
    "\n",
    "This is fairly simple on classification problem, as we create new examples adding a bit of noise to the features and keeping the same label. On regression problem this is a little more complex, as we also need to create new target values (we can use a previously trained model as in this [tutorial](https://machinelearningmastery.com/test-time-augmentation-with-scikit-learn/)).\n",
    "\n",
    "Generally it will be better to add noise direcly into the model (e.g. inside the ANN) but with scikit-learn this is not possible. To achieve a better or equivalent result we sould perform the trainign semi-manually using partial_fit() and changing the noise in in the features at each training iteration.\n",
    "\n",
    "Another approach is to add noise directly into the the raw-data before comuting the features, which usually help to improve the robustness against noise of the ML-based system.\n",
    "\n",
    "For more details on this you can read the following posts:\n",
    "\n",
    "* [Train Neural Networks With Noise to Reduce Overfitting](https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/)\n",
    "<!-- blank line -->\n",
    "\n",
    "* [How to Improve Deep Learning Model Robustness by Adding Noise](https://towardsdatascience.com/how-to-use-noise-to-your-advantage-5301071d9dc3)\n",
    "<!-- blank line -->\n",
    "\n",
    "* [How to use Noise to your advantage?](https://towardsdatascience.com/how-to-use-noise-to-your-advantage-5301071d9dc3)\n",
    "<!-- blank line -->\n",
    "\n",
    "* [Test-Time Augmentation For Tabular Data With Scikit-Learn](https://machinelearningmastery.com/test-time-augmentation-with-scikit-learn/)\n",
    "<!-- blank line -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 different classes: ['cello', 'guitar', 'clarinet', 'flute', 'harmonica']\n",
      "number of files in database 60\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#loading files and extracting features\n",
    "metadata = pd.read_csv('./data/examples4/meta.csv')\n",
    "classes = list(metadata.label.unique())\n",
    "print('There are',len(classes),'different classes:',classes)\n",
    "\n",
    "sr = 22050\n",
    "\n",
    "def extract_features(filename, sr):\n",
    "    signal, dummy = librosa.load(filename, sr=sr, mono=True)\n",
    "    output = np.mean(librosa.feature.mfcc(y=signal, n_mfcc=20), axis=1)\n",
    "    return output\n",
    "\n",
    "print('number of files in database',len(metadata.index))\n",
    "features = np.zeros((len(metadata.index),20))\n",
    "labels = np.zeros((len(metadata.index)))\n",
    "\n",
    "for i, row in metadata.iterrows():\n",
    "    features[i,:] = extract_features('./data/examples4/'+row['filename'], sr=sr)\n",
    "    labels[i] = (classes.index(row['label']))\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#creating pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('dim_red', PCA(n_components = 14)),\n",
    "        ('classifier', MLPClassifier(hidden_layer_sizes=(12,4), max_iter=10000, activation='tanh'))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extending a single split and using it to train the pipeline once\n",
    "\n",
    "Here we also train the same pipeline with the non extended dataset to show the difference.\n",
    "Mind that although we fixed the random split initializing the random_state to a fixed integer, the results (accuracy) will change at every execution. In particular, the weights of the ANN are initialized randomly, therefore reteaining again the ANN on the same dataset (non extended) may produce different results. At times the random initialization of weights may be more favorable to the task, other time less favorable. Moreover, when extendin the training set we add noise which is random by nature, and therefore also this will be at times favorable to the classification tasks, and at time adverse (as we will see in the next section, on average over a long run it should be favorable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 20)\n",
      "(96, 20)\n",
      "(48,)\n",
      "(96,)\n",
      "Accuracy without extended set: 0.5\n",
      "Accuracy with extended set: 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "#splitting in training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "feat_train, feat_test, lab_train, lab_test = train_test_split(features, labels, test_size=0.2, random_state=10)\n",
    "\n",
    "#the estended feature set include the original features\n",
    "#plus features with added gaussian noise (appending one extended array after the original)\n",
    "feat_train_ext = np.append(feat_train,feat_train+np.random.normal(0,0.15,(feat_train.shape)),axis=0)\n",
    "\n",
    "#as a consequence we also have to extend (doubling) the array of labels\n",
    "lab_train_ext = np.append(lab_train,lab_train,axis=0)\n",
    "\n",
    "#displaying the size of the original and extended arrays\n",
    "print(feat_train.shape)\n",
    "print(feat_train_ext.shape)\n",
    "print(lab_train.shape)\n",
    "print(lab_train_ext.shape)\n",
    "\n",
    "#training the pipe and checking the accuracy \n",
    "pipe.fit(feat_train, lab_train)\n",
    "lab_predict = pipe.predict(feat_test)\n",
    "print('Accuracy without extended set:',sklearn.metrics.accuracy_score(lab_test, lab_predict))\n",
    "\n",
    "#training the pipe and checking the accuracy on extended set\n",
    "pipe.fit(feat_train_ext, lab_train_ext)\n",
    "lab_predict = pipe.predict(feat_test)\n",
    "print('Accuracy with extended set:',sklearn.metrics.accuracy_score(lab_test, lab_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean and variance 0.7633333333333333 0.011211111111111111 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "#creating in the repeated stratified k-fold object\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "#empty list to store over the various iterations\n",
    "accuracy = []\n",
    "\n",
    "#iterating over the splits in the repeated stratified k-fold object (given our features and labels)\n",
    "for train_index, test_index in rskf.split(features, labels):\n",
    "    #creating features splits manually from indexes k fold indexes\n",
    "    feat_train, feat_test = features[train_index], features[test_index] \n",
    "    #creating labels splits manually from indexes k fold indexes\n",
    "    lab_train, lab_test = labels[train_index], labels[test_index]\n",
    "    #extending features by adding noise\n",
    "    feat_train_ext = np.append(feat_train,feat_train+np.random.normal(0,0.15,(feat_train.shape)),axis=0) #extending by adding noise\n",
    "    #extending labels\n",
    "    lab_train_ext = np.append(lab_train,lab_train,axis=0)\n",
    "    #training\n",
    "    pipe.fit(feat_train_ext, lab_train_ext)\n",
    "    #inference of test set\n",
    "    lab_predict = pipe.predict(feat_test)\n",
    "    #computing accuracy and adding to array (later compute mean and variance)\n",
    "    accuracy.append(sklearn.metrics.accuracy_score(lab_test, lab_predict)) \n",
    "\n",
    "print('Accuracy mean and variance', np.mean(accuracy),np.var(accuracy),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow up activity\n",
    "\n",
    "Take a classifier you have previously trained, in which you suspect that overfitting was an issue (perhaps try to plot the decision boundaries). Make an attempt to add noise manually to see if this improved performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5ed5c5ee634c1a3e3845d40e3c337718ae5d9e8873cda59b583427ad3ddde61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

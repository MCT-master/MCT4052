{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCT4052 Workshop 6d: Pipelines of Transforms\n",
    "\n",
    "*Author: Stefano Fasciani, stefano.fasciani@imv.uio.no, Department of Musicology, University of Oslo.*\n",
    "\n",
    "In this example we introduce the scikitlearn transformation [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), which beside being a useful tool to produce more code compact and less error prone ML programs, are also essential to perform cross validation and extensive grid search (introduced in following notebooks).\n",
    "\n",
    "Most models/objects available in scikit-learn (related to ML and associated utilities) present the methods .fit(x) and .transform(x)/predict(x). These may also present the method fit_transform(x) (usually not uded in the notebooks). In most ML examples analyzed so far we perform a sequence of transformations using .transform(x)/predict(x) methods of objects previously trained with the function .fit(x). When training, we always use the same split of the dataset (i.e. the training split). For example, a common sequence we used is scaler->dimensionality_reduction->classifier. Sequences can include more or less transformation stages (not necessarily three of them).\n",
    "\n",
    "Pipelines allows to create one macro object which embeds all transformation, which can be trained (i.e. fit) and used for inference (i.e. transform and/or predict) in a single line of code. In this example we use the pipeline on a supervised machine learning tasks, but it can be used for unsupervised as well as for any chain/sequence of scikit learn models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa, librosa.display\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 different classes: ['cello', 'guitar', 'clarinet', 'flute', 'harmonica']\n",
      "number of files in database 60\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#loading files and extracting features\n",
    "metadata = pd.read_csv('./data/examples4/meta.csv')\n",
    "classes = list(metadata.label.unique())\n",
    "print('There are',len(classes),'different classes:',classes)\n",
    "\n",
    "sr = 22050\n",
    "\n",
    "def extract_features(filename, sr):\n",
    "    signal, dummy = librosa.load(filename, sr=sr, mono=True)\n",
    "    output = np.mean(librosa.feature.mfcc(y=signal, n_mfcc=20), axis=1)\n",
    "    return output\n",
    "\n",
    "print('number of files in database',len(metadata.index))\n",
    "features = np.zeros((len(metadata.index),20))\n",
    "labels = np.zeros((len(metadata.index)))\n",
    "\n",
    "for i, row in metadata.iterrows():\n",
    "    features[i,:] = extract_features('./data/examples4/'+row['filename'], sr=sr)\n",
    "    labels[i] = (classes.index(row['label']))\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#splitting the dataset in training and testing parts\n",
    "feat_train, feat_test, lab_train, lab_test = train_test_split(features, labels, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating the pipeline and initializing parameters to inner objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('dim_red', PCA(n_components = 10)),\n",
    "        ('classifier', MLPClassifier(hidden_layer_sizes=(5,5), max_iter=10000, activation='relu'))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training and using the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled samples 3 out of 12\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "#training the pipeline\n",
    "pipe.fit(feat_train, lab_train)\n",
    "\n",
    "#applying the trained pipeline\n",
    "lab_predict = pipe.predict(feat_test)\n",
    "\n",
    "\n",
    "#print the number of misclassified samples, accuracy and complete report (using scikit learn metric tools) \n",
    "print('Number of mislabeled samples %d out of %d' % ((lab_test != lab_predict).sum(),lab_test.size))\n",
    "print('Accuracy:',sklearn.metrics.accuracy_score(lab_test, lab_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Follow up activity\n",
    "\n",
    "Use a transformation pipeline with one of the ML application you previously worked on using your own dataset. The transformation pipeline must include at least 3 components. Verify that the results are the same as when not using the transformation pipeline (use the same the random state in all components including random initializations).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2d1457ce15b0056f9f79091955f3674c195692e2c003e222eb6becd54b9e41f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCT4052 Workshop 6b: Saving and Restoring Trained ML Models with Joblib\n",
    "\n",
    "*Author: Stefano Fasciani, stefano.fasciani@imv.uio.no, Department of Musicology, University of Oslo.*\n",
    "\n",
    "When working with machine learning it is important to be able to save a trained model for later use, because training may be time consuming, or because we want to compare a collection of trained models. In this example we \"save to file\" and \"restore from file\" the models for scaling, dimensionality reduction and classifier. These models are needed when deploying ML in real-world application. The method detailed in this notebook works with all scikit-learn ML models. The models (i.e. Python objests) are saved/restored to/from files using the [Joblib package](https://joblib.readthedocs.io/en/latest/), which can be also used to save any other object in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa, librosa.display\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.style as ms\n",
    "#ms.use(\"seaborn-v0_8\")\n",
    "import IPython.display as Ipd\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sr = 22050\n",
    "\n",
    "def lin_interp_2d(data, out_size):\n",
    "    \n",
    "    x_in_size = data.shape[1]\n",
    "    y_in_size = data.shape[0]\n",
    "    x_in = np.arange(0,x_in_size)\n",
    "    y_in = np.arange(0,y_in_size)\n",
    "    interpolator = scipy.interpolate.RegularGridInterpolator((x_in, y_in), data.T, method='linear', bounds_error=False)\n",
    "    x_out = np.arange(0,x_in_size-1,((x_in_size-1)/out_size[1]))\n",
    "    y_out = np.arange(0,y_in_size-1,((y_in_size-1)/out_size[0]))\n",
    "    x_out, y_out = np.meshgrid(x_out, y_out, indexing='ij', sparse=True)\n",
    "    output = interpolator((x_out, y_out)).T\n",
    "    output = output[0:out_size[0],0:out_size[1]]\n",
    "\n",
    "    return output\n",
    "\n",
    "def extract_features(filename, sr):\n",
    "    \n",
    "    signal, dummy = librosa.load(filename, sr=sr, mono=True)\n",
    "    temp = librosa.feature.melspectrogram(y=signal, n_mels=60)\n",
    "    melspect1 = lin_interp_2d(temp, (50,10))\n",
    "    output = melspect1.flatten()\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "filenames = os.listdir('./data/examples2')\n",
    "features = np.zeros((len(filenames),500))\n",
    "labels = np.zeros((len(filenames)))\n",
    "classes = ['kick','snare','cymbal','clap'] \n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    features[i,:] = extract_features('./data/examples2/'+filenames[i], sr=sr)\n",
    "    if filenames[i].find('kick') != -1:\n",
    "        labels[i] = 0\n",
    "    elif filenames[i].find('snare') != -1:\n",
    "        labels[i] = 1\n",
    "    elif filenames[i].find('cymbal') != -1:\n",
    "        labels[i] = 2\n",
    "    elif filenames[i].find('clap') != -1:\n",
    "        labels[i] = 3\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#splitting the dataset in training and testing parts\n",
    "feat_train, feat_test, lab_train, lab_test = train_test_split(features, labels, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Saving and restoring all trained models to/from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning the scaling transformation from the train data and applying it to both train and test set.\n",
    "\n",
    "#creating scaling object\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "#learning scaling from train set\n",
    "scaler.fit(feat_train)\n",
    "\n",
    "#saving the scaler model to file\n",
    "joblib_file = \"scaler_model.pkl\"\n",
    "joblib.dump(scaler, joblib_file)\n",
    "\n",
    "#restoring the scaler model from file\n",
    "restored_scaler = joblib.load(joblib_file)\n",
    "\n",
    "#applying scaling to both train and test set\n",
    "feat_train = restored_scaler.transform(feat_train)\n",
    "feat_test = restored_scaler.transform(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an instance of the LDA object, which is an object capable of learning and applying LDA from/to data.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "#This will learn LDA projection from train data\n",
    "lda.fit(feat_train,lab_train)\n",
    "\n",
    "#saving the dimensionality reduction model to file\n",
    "joblib_file = \"dimred_model.pkl\"\n",
    "joblib.dump(lda, joblib_file)\n",
    "\n",
    "#restoring the dimensionality reduction model from file\n",
    "restored_lda = joblib.load(joblib_file)\n",
    "\n",
    "#Now we project the data using LDA\n",
    "projected_features_train = restored_lda.transform(feat_train)\n",
    "projected_features_test = restored_lda.transform(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled samples 11 out of 34\n",
      "Accuracy: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "#Creating an instance of a SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', C=2.0)\n",
    "\n",
    "#training the model\n",
    "svm.fit(projected_features_train, lab_train)\n",
    "\n",
    "#saving the classifier model to file\n",
    "joblib_file = \"class_model.pkl\"\n",
    "joblib.dump(svm, joblib_file)\n",
    "\n",
    "#restoring the classifier model from file\n",
    "restored_svm = joblib.load(joblib_file)\n",
    "\n",
    "#applying the the model on the test data (features)\n",
    "lab_predict = restored_svm.predict(projected_features_test)\n",
    "\n",
    "\n",
    "#print the number of misclassified samples, accuracy and complete report (using scikit learn metric tools) \n",
    "print('Number of mislabeled samples %d out of %d' % ((lab_test != lab_predict).sum(),lab_test.size))\n",
    "print('Accuracy:',sklearn.metrics.accuracy_score(lab_test, lab_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2d1457ce15b0056f9f79091955f3674c195692e2c003e222eb6becd54b9e41f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
